{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Pepsi and Coca Cola datasets\n",
    "pep = pd.read_csv('data/market_data/KO.csv')\n",
    "ko = pd.read_csv('data/market_data/PEP.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the datasets\n",
    "df = pd.merge(left=ko,right=pep,left_on='dt',right_on='dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating difference between lowest and highest price at that timestep for each company separately\n",
    "df['x_low_high_diff'] = df.high_x - df.low_x\n",
    "df['y_low_high_diff'] = df.high_y - df.low_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If I try predicting each company's stock price separately will face the problem of seasonality. \n",
    "#Thats's why I calculate ratio of a few variables between the 2 similar companies and later will try to predict the ratio's grouth\n",
    "\n",
    "df['close_ratio'] = df.close_x/df.close_y\n",
    "df['open_ratio'] = df.open_x/df.open_y\n",
    "df['volume_ratio'] = df.volume_x/df.volume_y\n",
    "df['diff_ratio'] = df.x_low_high_diff/df.y_low_high_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dt'] = pd.to_datetime(df.dt)\n",
    "\n",
    "df['month'] = df.dt.dt.month\n",
    "df['year'] = df.dt.dt.year\n",
    "df['dow'] = df.dt.dt.dayofweek\n",
    "df['hour'] = df.dt.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['open_ratio_grouth'] = 1\n",
    "df['close_ratio_grouth'] = 1\n",
    "df['open_close_ratio_grouth'] = 1\n",
    "df['volume_ratio_grouth'] = 1\n",
    "df['diff_ratio_grouth'] = 1 \n",
    "\n",
    "df.loc[1:,'open_ratio_grouth'] = np.array(df['open_ratio'].iloc[1:])/np.array(df['open_ratio'].iloc[:-1])\n",
    "df.loc[1:,'close_ratio_grouth'] = np.array(df['close_ratio'].iloc[1:])/np.array(df['close_ratio'].iloc[:-1])\n",
    "df.loc[1:,'volume_ratio_grouth'] = np.array(df['volume_ratio'].iloc[1:])/np.array(df['volume_ratio'].iloc[:-1])\n",
    "df.loc[1:,'diff_ratio_grouth'] = np.array(df['diff_ratio'].iloc[1:])/np.array(df['diff_ratio'].iloc[:-1])\n",
    "\n",
    "\n",
    "df.loc[:,'open_close_ratio_grouth'] = df['close_ratio']/df['open_ratio']\n",
    "df.loc[:,'open_close_ratio_grouth'] = df['close_ratio']/df['open_ratio']\n",
    "\n",
    "\n",
    "df.drop(df.index[0],inplace=True,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subtract 1 from ratio_grouth, so if ratio decreases, ratio_grouth be negative. \n",
    "#It makes loss calculation easier and debugging, exploratory analysis more clear.  \n",
    "df.loc[:,['open_ratio_grouth','close_ratio_grouth','open_close_ratio_grouth']]-=1\n",
    "#because initial weights of tensorflow fully connected layers are a bit high for our target value,\n",
    "#in stead of setting custom smaller weights I decided to make the target a bit larger by multiplying with 10\n",
    "df.loc[:,'close_ratio_grouth']*=10\n",
    "\n",
    "#exploring data after artificial grouth\n",
    "ea_df = df[df.open_ratio>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df,columns=['month','year','dow','hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because open and close ratios can have special behaviour in the beginning of day or week,\n",
    "#I multiply ratio grouths with respective dummy variables\n",
    "df['org_9'] = df['open_ratio_grouth']*df['hour_9'] \n",
    "df['org_0_9'] = df['open_ratio_grouth']*df['hour_9']*df['dow_0'] \n",
    "df['crg_4_16'] = df['close_ratio_grouth']*df['hour_16']*df['dow_4']\n",
    "df['crg_16'] = df['close_ratio_grouth']*df['hour_16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-0.5,0.5))\n",
    "df[['org_9','org_0_9','high_x','low_x','high_y','low_y','close_ratio','open_close_ratio_grouth','volume_ratio_grouth','open_ratio','open_ratio_grouth']] = scaler.fit_transform(df[['org_9','org_0_9','high_x','low_x','high_y','low_y','close_ratio','open_close_ratio_grouth','volume_ratio_grouth','open_ratio','open_ratio_grouth']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the features to be used to make prediction \n",
    "cols = ['org_9','org_0_9','crg_4_16','crg_16','high_x','low_x','high_y','low_y', 'month_1', 'month_2',\n",
    "       'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8',\n",
    "       'month_9', 'month_10', 'month_11', 'month_12', 'year_2010', 'year_2011',\n",
    "       'year_2012', 'year_2013', 'year_2014', 'year_2015', 'year_2016',\n",
    "       'year_2017', 'dow_0', 'dow_1', 'dow_2', 'dow_3', 'dow_4', 'hour_9',\n",
    "       'hour_10', 'hour_11', 'hour_12', 'hour_13', 'hour_14', 'hour_15',\n",
    "       'hour_16','close_ratio','open_close_ratio_grouth','volume_ratio_grouth','open_ratio','open_ratio_grouth','close_ratio_grouth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "#number of features\n",
    "input_size = len(cols)\n",
    "#number of features to be used in the time step of the target value\n",
    "#As our goal is predicting close ratio grouth I also use the open_ratio and open_ratio_grouth of the target's time time step\n",
    "last_input_size = 2\n",
    "#how many timesteps to look back\n",
    "num_steps = 140\n",
    "#how many timesteps forward to predict - 26 is nearly a day ahead\n",
    "max_iter = 26\n",
    "num_lstm_layers = 2\n",
    "lstm_size = 68\n",
    "learning_rate = 0.00001\n",
    "epochs = 4\n",
    "\n",
    "data = df[cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(batch_size,num_steps,input_size,last_input_size):\n",
    "    inputs = tf.placeholder(shape=(batch_size,num_steps,input_size),dtype=tf.float32,name='inputs')\n",
    "    opens = tf.placeholder(shape=(batch_size,last_input_size),dtype=tf.float32,name='opens')\n",
    "    targets = tf.placeholder(shape=(batch_size,max_iter,1),dtype=tf.float32,name='targets')\n",
    "    is_training = tf.placeholder(dtype=tf.bool,name='is_training')\n",
    "    \n",
    "    return inputs,opens,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size,num_layers,batch_size):\n",
    "    \n",
    "    stacked_rnn = []\n",
    "    for layer in range(num_layers):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm,output_keep_prob=0.7)\n",
    "        stacked_rnn.append(drop)\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells=stacked_rnn, state_is_tuple=True)\n",
    "    \n",
    "    initial_state = cell.zero_state(batch_size,dtype=tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output(d_outputs,opens,in_size,out_size=1):\n",
    "    \n",
    "    d_outputs = tf.reshape(d_outputs,(-1,in_size))\n",
    "    \n",
    "    #MLP\n",
    "    x1 = tf.contrib.layers.fully_connected(d_outputs,int(1.4*in_size),activation_fn=None)\n",
    "    x1 = tf.layers.batch_normalization(x1)\n",
    "    #leaky rely\n",
    "    x1 = tf.maximum(x1,x1*0.2)\n",
    "    x1 = tf.nn.dropout(x1,0.7)\n",
    "    \n",
    "    #in this step we concat the timeseries output we target timestep's values\n",
    "    x2 = tf.contrib.layers.fully_connected(x1,in_size,activation_fn=None)\n",
    "    x2 = tf.layers.batch_normalization(x2)\n",
    "    x2 = tf.maximum(x2,x2*0.2)\n",
    "    x2 = tf.nn.dropout(x2,0.7)\n",
    "    \n",
    "    x3 = tf.contrib.layers.fully_connected(x2,int(0.5*in_size),activation_fn=None)\n",
    "    x3 = tf.layers.batch_normalization(x3)\n",
    "    x3 = tf.maximum(x3,x3*0.2)\n",
    "    x3 = tf.nn.dropout(x3,0.7)\n",
    "    \n",
    "    outputs = tf.contrib.layers.fully_connected(x3,out_size,activation_fn=None)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(outputs, targets, out_size=1):\n",
    "    \n",
    "    targets = tf.reshape(targets, tf.shape(outputs))\n",
    "    #simply calculating  squared difference\n",
    "    loss1 = tf.reduce_mean(tf.squared_difference(outputs,targets))\n",
    "    \n",
    "    #calculate number of deals where predicted grouth but we had decrease and vice-versa\n",
    "    z = tf.reshape(tf.multiply(outputs,targets),(-1,))\n",
    "    x=tf.zeros((tf.shape(outputs)[0],))\n",
    "    y=tf.ones((tf.shape(outputs)[0],))\n",
    "    bad_deals = tf.where(z > 0, x, y)\n",
    "\n",
    "    loss2 = tf.multiply(tf.reduce_mean(bad_deals),0.09)\n",
    "    \n",
    "    \n",
    "    loss = tf.add(loss1,loss2)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self,batch_size,num_steps,lstm_size,num_lstm_layers,last_input_size,input_size,learning_rate):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.inputs,self.opens,self.targets = build_inputs(batch_size,num_steps,input_size,last_input_size)\n",
    "        \n",
    "        cell, self.initial_state = build_lstm(lstm_size,num_lstm_layers,batch_size)\n",
    "        \n",
    "        _, state = tf.nn.dynamic_rnn(cell=cell,initial_state=self.initial_state,inputs=self.inputs)\n",
    "        \n",
    "        self.final_state = state\n",
    "        \n",
    "        self.d_outputs, _ = build_decoder(state[-1],self.targets)\n",
    "        \n",
    "        self.outputs = build_output(self.d_outputs,self.opens,lstm_size)\n",
    "        \n",
    "        self.loss = build_loss(self.outputs,self.targets,batch_size)\n",
    "        \n",
    "        self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network(batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_lstm_layers=num_lstm_layers, \n",
    "                last_input_size = last_input_size, input_size=input_size,learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    l=(len(data)-num_steps*batch_size-max_iter)//4\n",
    "    #passing thru all timesteps\n",
    "    for i in range(l):\n",
    "                \n",
    "        #collecting batch from 4 different time fields\n",
    "        j = len(data)//4\n",
    "        ids = list(range(i,(i+(max_iter+num_steps)*batch_size//4))) + list(range(j,(j+(max_iter+num_steps)*batch_size//4))) + list(range(-1*(j+(max_iter+num_steps)*batch_size//4)-1,-j-1)) + list(range(-1*(i+(max_iter+num_steps)*batch_size//4)-1,-i-1))\n",
    "        \n",
    "        batch = np.reshape(data[ids],(batch_size,num_steps+max_iter,input_size))\n",
    "\n",
    "        yield batch[:,:-max_iter],batch[:,-max_iter:,-4:-2],batch[:,-max_iter:,-1:],(l-i)<=100#x,x_open,y,test_batch(last 100 batches are used for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating proportion of deals where we managed to correctly predict if the ratio will rise or fall\n",
    "def won_deal_prop(y,y_hat):\n",
    "    y = np.reshape(y,(-1,1))\n",
    "    win_deals = sum((y_hat[:,0]*y[:,0])>0)\n",
    "    return win_deals,win_deals/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        total_train_loss,total_test_loss=0,0\n",
    "        total_train_won_deals,total_test_won_deals=0,0\n",
    "        \n",
    "        n_train_batches=0\n",
    "        n_test_batches=0\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        \n",
    "        for x, x_open, y, test_batch in get_batch():\n",
    "\n",
    "            feed = {model.inputs: x,model.opens:x_open,\n",
    "                    model.targets: y,model.initial_state:new_state\n",
    "                    }\n",
    "                   \n",
    "            fetch = [model.loss,model.final_state,model.outputs,model.opt,model.is_training:True]\n",
    "            if test_batch:\n",
    "                #Last batches must be only used for testing so setting another value just to escape from optimizing \n",
    "                fetch[-1]=model.d_outputs\n",
    "                feed[model.is_training]=False\n",
    "                \n",
    "            batch_loss, new_state, y_hat, _ = sess.run(fetch,feed_dict=feed)\n",
    "            batch_loss = batch_loss**0.5\n",
    "            win_deals = sum((y_hat[:,0]*y[:,0])>0)\n",
    "            win_deal_prop = win_deals/len(y)\n",
    "                         \n",
    "            if n_train_batches % 200 == 0 and not test_batch:\n",
    "                print(e,n_train_batches,'loss',batch_loss,'won deals',win_deal_prop)\n",
    "            \n",
    "            if test_batch:\n",
    "                total_test_loss+= batch_loss\n",
    "                total_test_won_deals+=win_deals\n",
    "                n_test_batches+=1  \n",
    "            else:\n",
    "                total_train_loss+= batch_loss\n",
    "                total_train_won_deals+=win_deals\n",
    "                n_train_batches+=1  \n",
    "                     \n",
    "        mean_test_loss=total_test_loss/n_test_batches    \n",
    "        print('Train loss',total_train_loss/n_train_batches, 'Train won deals prop',total_train_won_deals/(n_train_batches*len(y)))\n",
    "        print('Test loss',mean_test_loss, 'Test won deals prop',total_test_won_deals/(n_test_batches*len(y)))\n",
    "        path = 'checkpoints/long_run_e{0}_loss{1}'.format(e,mean_test_loss)\n",
    "        saver.save(sess,save_path=path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
