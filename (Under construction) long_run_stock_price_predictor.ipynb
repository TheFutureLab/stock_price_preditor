{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Pepsi and Coca Cola datasets\n",
    "pep = pd.read_csv('data/market_data/KO.csv')\n",
    "ko = pd.read_csv('data/market_data/PEP.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the datasets\n",
    "df = pd.merge(left=ko,right=pep,left_on='dt',right_on='dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating difference between lowest and highest price at that timestep for each company separately\n",
    "df['x_low_high_diff'] = df.high_x - df.low_x\n",
    "df['y_low_high_diff'] = df.high_y - df.low_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If I try predicting each company's stock price separately will face the problem of seasonality. \n",
    "#Thats's why I calculate ratio of a few variables between the 2 similar companies and later will try to predict the ratio's grouth\n",
    "\n",
    "df['close_ratio'] = df.close_x/df.close_y\n",
    "df['open_ratio'] = df.open_x/df.open_y\n",
    "df['volume_ratio'] = df.volume_x/df.volume_y\n",
    "df['diff_ratio'] = df.x_low_high_diff/df.y_low_high_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dt'] = pd.to_datetime(df.dt)\n",
    "\n",
    "df['month'] = df.dt.dt.month\n",
    "df['year'] = df.dt.dt.year\n",
    "df['dow'] = df.dt.dt.dayofweek\n",
    "df['hour'] = df.dt.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "df['open_ratio_grouth'] = 1\n",
    "df['close_ratio_grouth'] = 1\n",
    "df['open_close_ratio_grouth'] = 1\n",
    "df['volume_ratio_grouth'] = 1\n",
    "df['diff_ratio_grouth'] = 1 \n",
    "\n",
    "df.loc[1:,'open_ratio_grouth'] = np.array(df['open_ratio'].iloc[1:])/np.array(df['open_ratio'].iloc[:-1])\n",
    "df.loc[1:,'close_ratio_grouth'] = np.array(df['close_ratio'].iloc[1:])/np.array(df['close_ratio'].iloc[:-1])\n",
    "df.loc[1:,'volume_ratio_grouth'] = np.array(df['volume_ratio'].iloc[1:])/np.array(df['volume_ratio'].iloc[:-1])\n",
    "df.loc[1:,'diff_ratio_grouth'] = np.array(df['diff_ratio'].iloc[1:])/np.array(df['diff_ratio'].iloc[:-1])\n",
    "\n",
    "\n",
    "df.loc[:,'open_close_ratio_grouth'] = df['close_ratio']/df['open_ratio']\n",
    "df.loc[:,'open_close_ratio_grouth'] = df['close_ratio']/df['open_ratio']\n",
    "\n",
    "\n",
    "df.drop(df.index[0],inplace=True,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>open_x</th>\n",
       "      <th>high_x</th>\n",
       "      <th>low_x</th>\n",
       "      <th>close_x</th>\n",
       "      <th>volume_x</th>\n",
       "      <th>open_y</th>\n",
       "      <th>high_y</th>\n",
       "      <th>low_y</th>\n",
       "      <th>close_y</th>\n",
       "      <th>...</th>\n",
       "      <th>diff_ratio</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>dow</th>\n",
       "      <th>hour</th>\n",
       "      <th>open_ratio_grouth</th>\n",
       "      <th>close_ratio_grouth</th>\n",
       "      <th>open_close_ratio_grouth</th>\n",
       "      <th>volume_ratio_grouth</th>\n",
       "      <th>diff_ratio_grouth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-04 10:00:00</td>\n",
       "      <td>60.710</td>\n",
       "      <td>60.980</td>\n",
       "      <td>60.70</td>\n",
       "      <td>60.951</td>\n",
       "      <td>832628</td>\n",
       "      <td>57.000</td>\n",
       "      <td>57.1100</td>\n",
       "      <td>56.90</td>\n",
       "      <td>57.01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.994941</td>\n",
       "      <td>1.003959</td>\n",
       "      <td>1.003794</td>\n",
       "      <td>1.333854</td>\n",
       "      <td>0.751515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-04 10:15:00</td>\n",
       "      <td>60.950</td>\n",
       "      <td>61.099</td>\n",
       "      <td>60.87</td>\n",
       "      <td>61.055</td>\n",
       "      <td>1123591</td>\n",
       "      <td>57.005</td>\n",
       "      <td>57.1600</td>\n",
       "      <td>56.95</td>\n",
       "      <td>57.16</td>\n",
       "      <td>...</td>\n",
       "      <td>1.090476</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1.003865</td>\n",
       "      <td>0.999078</td>\n",
       "      <td>0.999006</td>\n",
       "      <td>0.986506</td>\n",
       "      <td>0.817857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-04 10:30:00</td>\n",
       "      <td>61.055</td>\n",
       "      <td>61.170</td>\n",
       "      <td>61.02</td>\n",
       "      <td>61.140</td>\n",
       "      <td>1387369</td>\n",
       "      <td>57.160</td>\n",
       "      <td>57.2200</td>\n",
       "      <td>57.11</td>\n",
       "      <td>57.21</td>\n",
       "      <td>...</td>\n",
       "      <td>1.363636</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.999006</td>\n",
       "      <td>1.000517</td>\n",
       "      <td>1.000517</td>\n",
       "      <td>1.004114</td>\n",
       "      <td>1.250496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-04 10:45:00</td>\n",
       "      <td>61.140</td>\n",
       "      <td>61.180</td>\n",
       "      <td>61.04</td>\n",
       "      <td>61.062</td>\n",
       "      <td>1644542</td>\n",
       "      <td>57.200</td>\n",
       "      <td>57.2200</td>\n",
       "      <td>57.11</td>\n",
       "      <td>57.14</td>\n",
       "      <td>...</td>\n",
       "      <td>1.272727</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000692</td>\n",
       "      <td>0.999948</td>\n",
       "      <td>0.999773</td>\n",
       "      <td>1.036124</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2010-01-04 11:00:00</td>\n",
       "      <td>61.070</td>\n",
       "      <td>61.140</td>\n",
       "      <td>61.00</td>\n",
       "      <td>61.010</td>\n",
       "      <td>2001837</td>\n",
       "      <td>57.140</td>\n",
       "      <td>57.1685</td>\n",
       "      <td>57.05</td>\n",
       "      <td>57.09</td>\n",
       "      <td>...</td>\n",
       "      <td>1.181435</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.999904</td>\n",
       "      <td>1.000023</td>\n",
       "      <td>0.999892</td>\n",
       "      <td>1.084119</td>\n",
       "      <td>0.928270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   dt  open_x  high_x  low_x  close_x  volume_x  open_y  \\\n",
       "1 2010-01-04 10:00:00  60.710  60.980  60.70   60.951    832628  57.000   \n",
       "2 2010-01-04 10:15:00  60.950  61.099  60.87   61.055   1123591  57.005   \n",
       "3 2010-01-04 10:30:00  61.055  61.170  61.02   61.140   1387369  57.160   \n",
       "4 2010-01-04 10:45:00  61.140  61.180  61.04   61.062   1644542  57.200   \n",
       "5 2010-01-04 11:00:00  61.070  61.140  61.00   61.010   2001837  57.140   \n",
       "\n",
       "    high_y  low_y  close_y        ...          diff_ratio  month  year  dow  \\\n",
       "1  57.1100  56.90    57.01        ...            1.333333      1  2010    0   \n",
       "2  57.1600  56.95    57.16        ...            1.090476      1  2010    0   \n",
       "3  57.2200  57.11    57.21        ...            1.363636      1  2010    0   \n",
       "4  57.2200  57.11    57.14        ...            1.272727      1  2010    0   \n",
       "5  57.1685  57.05    57.09        ...            1.181435      1  2010    0   \n",
       "\n",
       "   hour  open_ratio_grouth  close_ratio_grouth  open_close_ratio_grouth  \\\n",
       "1    10           0.994941            1.003959                 1.003794   \n",
       "2    10           1.003865            0.999078                 0.999006   \n",
       "3    10           0.999006            1.000517                 1.000517   \n",
       "4    10           1.000692            0.999948                 0.999773   \n",
       "5    11           0.999904            1.000023                 0.999892   \n",
       "\n",
       "   volume_ratio_grouth  diff_ratio_grouth  \n",
       "1             1.333854           0.751515  \n",
       "2             0.986506           0.817857  \n",
       "3             1.004114           1.250496  \n",
       "4             1.036124           0.933333  \n",
       "5             1.084119           0.928270  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subtract 1 from ratio_grouth, so if ratio decreases, ratio_grouth be negative. \n",
    "#It makes loss calculation easier and debugging, exploratory analysis more clear.  \n",
    "df.loc[:,['open_ratio_grouth','close_ratio_grouth','open_close_ratio_grouth']]-=1\n",
    "#because initial weights of tensorflow fully connected layers are a bit high for our target value,\n",
    "#in stead of setting custom smaller weights I decided to make the target a bit larger by multiplying with 10\n",
    "df.loc[:,'close_ratio_grouth']*=10\n",
    "\n",
    "#exploring data after artificial grouth\n",
    "ea_df = df[df.open_ratio>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df,columns=['month','year','dow','hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because open and close ratios can have special behaviour in the beginning of day or week,\n",
    "#I multiply ratio grouths with respective dummy variables\n",
    "df['org_9'] = df['open_ratio_grouth']*df['hour_9'] \n",
    "df['org_0_9'] = df['open_ratio_grouth']*df['hour_9']*df['dow_0'] \n",
    "df['crg_4_16'] = df['close_ratio_grouth']*df['hour_16']*df['dow_4']\n",
    "df['crg_16'] = df['close_ratio_grouth']*df['hour_16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-0.5,0.5))\n",
    "df[['org_9','org_0_9','high_x','low_x','high_y','low_y','close_ratio','open_close_ratio_grouth','volume_ratio_grouth','open_ratio','open_ratio_grouth']] = scaler.fit_transform(df[['org_9','org_0_9','high_x','low_x','high_y','low_y','close_ratio','open_close_ratio_grouth','volume_ratio_grouth','open_ratio','open_ratio_grouth']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the features to be used to make prediction \n",
    "cols = ['org_9','org_0_9','crg_4_16','crg_16','high_x','low_x','high_y','low_y', 'month_1', 'month_2',\n",
    "       'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8',\n",
    "       'month_9', 'month_10', 'month_11', 'month_12', 'year_2010', 'year_2011',\n",
    "       'year_2012', 'year_2013', 'year_2014', 'year_2015', 'year_2016',\n",
    "       'year_2017', 'dow_0', 'dow_1', 'dow_2', 'dow_3', 'dow_4', 'hour_9',\n",
    "       'hour_10', 'hour_11', 'hour_12', 'hour_13', 'hour_14', 'hour_15',\n",
    "       'hour_16','close_ratio','open_close_ratio_grouth','volume_ratio_grouth','open_ratio','open_ratio_grouth','close_ratio_grouth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "#number of features\n",
    "input_size = len(cols)\n",
    "#number of features to be used in the time step of the target value\n",
    "#As our goal is predicting close ratio grouth I also use the open_ratio and open_ratio_grouth of the target's time time step\n",
    "last_input_size = 2\n",
    "#how many timesteps to look back\n",
    "num_steps = 140\n",
    "#how many timesteps forward to predict - 26 is nearly a day ahead\n",
    "max_iter = 26\n",
    "num_lstm_layers = 2\n",
    "lstm_size = 68\n",
    "learning_rate = 0.00001\n",
    "epochs = 4\n",
    "\n",
    "data = df[cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(batch_size,num_steps,input_size,last_input_size):\n",
    "    inputs = tf.placeholder(shape=(batch_size,num_steps,input_size),dtype=tf.float32,name='inputs')\n",
    "    opens = tf.placeholder(shape=(batch_size,last_input_size),dtype=tf.float32,name='opens')\n",
    "    targets = tf.placeholder(shape=(batch_size,max_iter,1),dtype=tf.float32,name='targets')\n",
    "    is_training = tf.placeholder(dtype=tf.bool,name='is_training')\n",
    "    \n",
    "    return inputs,opens,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size,num_layers,batch_size):\n",
    "    \n",
    "    stacked_rnn = []\n",
    "    for layer in range(num_layers):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm,output_keep_prob=0.7)\n",
    "        stacked_rnn.append(drop)\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells=stacked_rnn, state_is_tuple=True)\n",
    "    \n",
    "    initial_state = cell.zero_state(batch_size,dtype=tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output(d_outputs,opens,in_size,out_size=1):\n",
    "    \n",
    "    d_outputs = tf.reshape(d_outputs,(-1,in_size))\n",
    "    \n",
    "    #MLP\n",
    "    x1 = tf.contrib.layers.fully_connected(d_outputs,int(1.4*in_size),activation_fn=None)\n",
    "    x1 = tf.layers.batch_normalization(x1)\n",
    "    #leaky rely\n",
    "    x1 = tf.maximum(x1,x1*0.2)\n",
    "    x1 = tf.nn.dropout(x1,0.7)\n",
    "    \n",
    "    #in this step we concat the timeseries output we target timestep's values\n",
    "    x2 = tf.contrib.layers.fully_connected(x1,in_size,activation_fn=None)\n",
    "    x2 = tf.layers.batch_normalization(x2)\n",
    "    x2 = tf.maximum(x2,x2*0.2)\n",
    "    x2 = tf.nn.dropout(x2,0.7)\n",
    "    \n",
    "    x3 = tf.contrib.layers.fully_connected(x2,int(0.5*in_size),activation_fn=None)\n",
    "    x3 = tf.layers.batch_normalization(x3)\n",
    "    x3 = tf.maximum(x3,x3*0.2)\n",
    "    x3 = tf.nn.dropout(x3,0.7)\n",
    "    \n",
    "    outputs = tf.contrib.layers.fully_connected(x3,out_size,activation_fn=None)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(outputs, targets, out_size=1):\n",
    "    \n",
    "    targets = tf.reshape(targets, tf.shape(outputs))\n",
    "    #simply calculating  squared difference\n",
    "    loss1 = tf.reduce_mean(tf.squared_difference(outputs,targets))\n",
    "    \n",
    "    #calculate number of deals where predicted grouth but we had decrease and vice-versa\n",
    "    z = tf.reshape(tf.multiply(outputs,targets),(-1,))\n",
    "    x=tf.zeros((tf.shape(outputs)[0],))\n",
    "    y=tf.ones((tf.shape(outputs)[0],))\n",
    "    bad_deals = tf.where(z > 0, x, y)\n",
    "\n",
    "    loss2 = tf.multiply(tf.reduce_mean(bad_deals),0.09)\n",
    "    \n",
    "    \n",
    "    loss = tf.add(loss1,loss2)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder(state,targets):\n",
    "    return NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self,batch_size,num_steps,lstm_size,num_lstm_layers,last_input_size,input_size,learning_rate):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.inputs,self.opens,self.targets = build_inputs(batch_size,num_steps,input_size,last_input_size)\n",
    "        \n",
    "        cell, self.initial_state = build_lstm(lstm_size,num_lstm_layers,batch_size)\n",
    "        \n",
    "        _, state = tf.nn.dynamic_rnn(cell=cell,initial_state=self.initial_state,inputs=self.inputs)\n",
    "        \n",
    "        self.final_state = state\n",
    "        print(state[-1])\n",
    "        self.d_outputs, _ = build_decoder(state[-1],self.targets)\n",
    "        \n",
    "        self.outputs = build_output(self.d_outputs,self.opens,lstm_size)\n",
    "        \n",
    "        self.loss = build_loss(self.outputs,self.targets,batch_size)\n",
    "        \n",
    "        self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_5:0' shape=(32, 68) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_6:0' shape=(32, 68) dtype=float32>)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NotImplementedType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-98b4b2403caf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model = Network(batch_size=batch_size, num_steps=num_steps,\n\u001b[1;32m      2\u001b[0m                 \u001b[0mlstm_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_lstm_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_lstm_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                 last_input_size = last_input_size, input_size=input_size,learning_rate=learning_rate)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-33fb328ea987>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_size, num_steps, lstm_size, num_lstm_layers, last_input_size, input_size, learning_rate)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NotImplementedType' object is not iterable"
     ]
    }
   ],
   "source": [
    "model = Network(batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_lstm_layers=num_lstm_layers, \n",
    "                last_input_size = last_input_size, input_size=input_size,learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    l=(len(data)-num_steps*batch_size-max_iter)//4\n",
    "    #passing thru all timesteps\n",
    "    for i in range(l):\n",
    "                \n",
    "        #collecting batch from 4 different time fields\n",
    "        j = len(data)//4\n",
    "        ids = list(range(i,(i+(max_iter+num_steps)*batch_size//4))) + list(range(j,(j+(max_iter+num_steps)*batch_size//4))) + list(range(-1*(j+(max_iter+num_steps)*batch_size//4)-1,-j-1)) + list(range(-1*(i+(max_iter+num_steps)*batch_size//4)-1,-i-1))\n",
    "        \n",
    "        batch = np.reshape(data[ids],(batch_size,num_steps+max_iter,input_size))\n",
    "\n",
    "        yield batch[:,:-max_iter],batch[:,-max_iter:,-4:-2],batch[:,-max_iter:,-1:],(l-i)<=100#x,x_open,y,test_batch(last 100 batches are used for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating proportion of deals where we managed to correctly predict if the ratio will rise or fall\n",
    "def won_deal_prop(y,y_hat):\n",
    "    y = np.reshape(y,(-1,1))\n",
    "    win_deals = sum((y_hat[:,0]*y[:,0])>0)\n",
    "    return win_deals,win_deals/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        total_train_loss,total_test_loss=0,0\n",
    "        total_train_won_deals,total_test_won_deals=0,0\n",
    "        \n",
    "        n_train_batches=0\n",
    "        n_test_batches=0\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        \n",
    "        for x, x_open, y, test_batch in get_batch():\n",
    "\n",
    "            feed = {model.inputs: x,model.opens:x_open,\n",
    "                    model.targets: y,model.initial_state:new_state\n",
    "                    }\n",
    "                   \n",
    "            fetch = [model.loss,model.final_state,model.outputs,model.opt,model.is_training:True]\n",
    "            if test_batch:\n",
    "                #Last batches must be only used for testing so setting another value just to escape from optimizing \n",
    "                fetch[-1]=model.d_outputs\n",
    "                feed[model.is_training]=False\n",
    "                \n",
    "            batch_loss, new_state, y_hat, _ = sess.run(fetch,feed_dict=feed)\n",
    "            batch_loss = batch_loss**0.5\n",
    "            win_deals = sum((y_hat[:,0]*y[:,0])>0)\n",
    "            win_deal_prop = win_deals/len(y)\n",
    "                         \n",
    "            if n_train_batches % 200 == 0 and not test_batch:\n",
    "                print(e,n_train_batches,'loss',batch_loss,'won deals',win_deal_prop)\n",
    "            \n",
    "            if test_batch:\n",
    "                total_test_loss+= batch_loss\n",
    "                total_test_won_deals+=win_deals\n",
    "                n_test_batches+=1  \n",
    "            else:\n",
    "                total_train_loss+= batch_loss\n",
    "                total_train_won_deals+=win_deals\n",
    "                n_train_batches+=1  \n",
    "                     \n",
    "        mean_test_loss=total_test_loss/n_test_batches    \n",
    "        print('Train loss',total_train_loss/n_train_batches, 'Train won deals prop',total_train_won_deals/(n_train_batches*len(y)))\n",
    "        print('Test loss',mean_test_loss, 'Test won deals prop',total_test_won_deals/(n_test_batches*len(y)))\n",
    "        path = 'checkpoints/long_run_e{0}_loss{1}'.format(e,mean_test_loss)\n",
    "        saver.save(sess,save_path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draft\n",
    "seq_length =  [max_iter for _ in range(batch_size)]\n",
    "    \n",
    "cell = tf.contrib.rnn.BasicLSTMCell(in_size)\n",
    "\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(targets,seq_length)\n",
    "    \n",
    "decoder = tf.contrib.seq2seq.BasicDecoder(cell=cell,helper=helper,initial_state=state[-1])\n",
    "d_outputs,_,_ = tf.contrib.seq2seq.dynamic_decode(decoder,max_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
